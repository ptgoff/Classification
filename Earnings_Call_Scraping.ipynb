{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping conference call transcripts from Seeking Alpha <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a searched-for stock:\n",
    "    https://seekingalpha.com/search?q=atra&type=keyword&tab=transcripts # 10 links per page. Not all are transcripts\n",
    "    <a class=\"item-ticker\" href=\"/symbol/ATRA\"><strong>ATRA</strong></a> # for each link, this can be used to identify the article as focused on the stock in question (rather than the stock referenced in another stock's report). \n",
    "    <a href=\"/author/sa-transcripts\">SA Transcripts</a> This can be used to ensure that the link is for a transcript (rather than a news article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each stock can be has a transcript page: https://seekingalpha.com/symbol/RARE/transcripts <br>\n",
    "However, these also include slideshows and presentation decks. <br>\n",
    "All earnings call transcripts are distinguished by the phrase \"Earnings Call Transcript\" within <a class=\"__29a76-3a6i8\" href=\"/article/4424388-ultragenyx-pharmaceutical-inc-s-rare-ceo-emil-kakkis-on-q1-2021-results-earnings-call?source=content_type%3Areact%7Csection%3ATranscripts%7Csection_asset%3ATranscripts%7Cfirst_level_url%3Asymbol%7Cbutton%3ATitle%7Clock_status%3ANo%7Cline%3A1\">Ultragenyx Pharmaceutical Inc.'s (RARE) CEO Emil Kakkis on Q1 2021 Results - Earnings Call Transcript</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it stands, my strategy will be to:\n",
    "* Use yfinance API to identify 1,000 stocks that (a) have been incorporated for 5 years and (b) are in the same sector. \n",
    "* Collect tickers from these stocks\n",
    "* Loop over the ticker labels, years, and quarters to scrape transcript data from Seeking Alpha\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T19:37:03.354327Z",
     "iopub.status.busy": "2021-05-11T19:37:03.354060Z",
     "iopub.status.idle": "2021-05-11T19:37:03.358643Z",
     "shell.execute_reply": "2021-05-11T19:37:03.357552Z",
     "shell.execute_reply.started": "2021-05-11T19:37:03.354302Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from numpy import random\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T11:58:39.633335Z",
     "iopub.status.busy": "2021-05-12T11:58:39.633126Z",
     "iopub.status.idle": "2021-05-12T11:58:46.127252Z",
     "shell.execute_reply": "2021-05-12T11:58:46.126496Z",
     "shell.execute_reply.started": "2021-05-12T11:58:39.633312Z"
    }
   },
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "#chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--enable-javascript\")\n",
    "driver = webdriver.Chrome(options=chrome_options, executable_path='/opt/anaconda3/envs/metis/lib/python3.8/site-packages/chromedriver_binary/chromedriver')\n",
    "url = 'https://seekingalpha.com/account/login'\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.add_cookie({\"name\": \"key\", \"value\": \"value\"})\n",
    "\n",
    "# Entering my credentials\n",
    "elements = driver.find_elements_by_xpath('//*[@id=\"root\"]/div[1]/main/div[2]/form/div/div[1]/label/input')\n",
    "username = elements[0]\n",
    "username.clear()\n",
    "user_name = \"ptgoff1.618@gmail.com\"\n",
    "username.send_keys(user_name)\n",
    "\n",
    "elements = driver.find_elements_by_xpath('//*[@id=\"signInPasswordField\"]')\n",
    "password = elements[0]\n",
    "password.clear()\n",
    "password.send_keys(\"0sYO3@Ju4NiV\")\n",
    "\n",
    "elements = driver.find_elements_by_xpath('//*[@id=\"root\"]/div[1]/main/div[2]/form/button')\n",
    "submit = elements[0]\n",
    "submit.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T11:59:00.274778Z",
     "iopub.status.busy": "2021-05-12T11:59:00.274513Z",
     "iopub.status.idle": "2021-05-12T12:02:53.094234Z",
     "shell.execute_reply": "2021-05-12T12:02:53.093401Z",
     "shell.execute_reply.started": "2021-05-12T11:59:00.274748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NS-PC 0 transcripts recieved\n",
      "ALIN-PA 0 transcripts recieved\n",
      "ALIN-PE 0 transcripts recieved\n",
      "EP-PC 0 transcripts recieved\n",
      "GLOP-PA 0 transcripts recieved\n",
      "CEQP-P 0 transcripts recieved\n",
      "HMLP-PA 0 transcripts recieved\n",
      "ALIN-PB 0 transcripts recieved\n",
      "DCP-PC 0 transcripts recieved\n",
      "NS-PA 0 transcripts recieved\n",
      "VAL 12 transcripts recieved\n",
      "TGP-PB 0 transcripts recieved\n",
      "DLNG-PA 0 transcripts recieved\n",
      "GLP-PB 0 transcripts recieved\n",
      "GLP-PA 0 transcripts recieved\n",
      "GMLPP 0 transcripts recieved\n",
      "UROY 0 transcripts recieved\n"
     ]
    }
   ],
   "source": [
    "energy_tickers = [\"XOM\" ,\"CVX\" ,\"RDS-A\" ,\"RDS-B\" ,\"PTR\" ,\"TOT\" ,\"BP\" ,\"ENB\" ,\"SNP\" ,\"COP\" ,\"EQNR\" ,\"PBR-A\" ,\"PBR\" ,\"EPD\" ,\"TRP\" ,\n",
    "                  \"EOG\" ,\"E\" ,\"SLB\" ,\"KMI\" ,\"CNQ\" ,\"MPC\" ,\"PSX\" ,\"PXD\" ,\"SU\" ,\"VLO\" ,\"WMB\" ,\"CSAN\" ,\"MPLX\" ,\"HES\" ,\"ET\" ,\"OXY\" ,\n",
    "                  \"EC\" ,\"BKR\" ,\"OKE\" ,\"HAL\" ,\"PBA\" ,\"DVN\" ,\"CVE\" ,\"TS\" ,\"FANG\" ,\"TPL\" ,\"CLR\" ,\"MMP\" ,\"SSL\" ,\"MRO\" ,\"WES\" ,\"TRGP\" ,\n",
    "                  \"APA\" ,\"PSXP\" ,\"CCJ\" ,\"XEC\" ,\"PAA\" ,\"COG\" ,\"OVV\" ,\"NOV\" ,\"SHLX\" ,\"VVV\" ,\"HFC\" ,\"SHI\" ,\"DCP\" ,\"EQT\" ,\"CHX\" ,\"NFG\" ,\n",
    "                  \"CHK\" ,\"AM\" ,\"UGP\" ,\"PDCE\" ,\"FTI\" ,\"SUN\" ,\"ETRN\" ,\"ENBL\" ,\"MTDR\" ,\"SWN\" ,\"HP\" ,\"MGY\" ,\"CNX\" ,\"VNOM\" ,\"RRC\" ,\"DEN\" ,\n",
    "                  \"MUR\" ,\"AR\" ,\"NS-PB\" ,\"WHD\" ,\"ENLC\" ,\"LBRT\" ,\"REGI\" ,\"CPG\" ,\"CVI\" ,\"RIG\" ,\"CEQP\" ,\"BSM\" ,\"HEP\" ,\"INT\" ,\"NS\" ,\"CRC\" ,\n",
    "                  \"SM\" ,\"CLNE\" ,\"PBF\" ,\"DK\" ,\"PAGP\" ,\"DKL\" ,\"EURN\" ,\"RTLR\" ,\"CPE\" ,\"OAS\" ,\"WLL\" ,\"FRO\" ,\"PTEN\" ,\"YPF\" ,\"AROC\" ,\"USAC\" ,\n",
    "                  \"BPMP\" ,\"CLB\" ,\"OII\" ,\"GLOG-PA\" ,\"NBLX\" ,\"CRK\" ,\"TGP\" ,\"ERF\" ,\"DNOW\" ,\"DRQ\" ,\"VET\" ,\"GLNG\" ,\"RES\" ,\"KOS\" ,\"GEL\" ,\n",
    "                  \"XOG\" ,\"PUMP\" ,\"STNG\" ,\"NGL-PB\" ,\"NGL-PC\" ,\"VEI\" ,\"TALO\" ,\"DHT\" ,\"MNRL\" ,\"PBFX\" ,\"GPRK\" ,\"GLOP-PC\" ,\"GLOP-PB\" ,\"MRC\" ,\n",
    "                  \"GLP\" ,\"NEX\" ,\"VTOL\" ,\"FI\" ,\"SLCA\" ,\"BOOM\" ,\"PARR\" ,\"BCEI\" ,\"OMP\" ,\"ARCH\" ,\"CAPL\" ,\"ARLP\" ,\"HLX\" ,\"KRP\" ,\"ESTE\" ,\"LPG\" ,\n",
    "                  \"NBR\" ,\"TRMD\" ,\"TGS\" ,\"FLNG\" ,\"WTTR\" ,\"NVGS\" ,\"PVAC\" ,\"TDW\" ,\"GLOG\" ,\"HMLP\" ,\"SRLP\" ,\"NBR-PA\" ,\"DMLP\" ,\"SBR\" ,\"CLMT\" ,\n",
    "                  \"REX\" ,\"BRY\" ,\"WTI\" ,\"TNK\" ,\"SOI\" ,\"BTU\" ,\"DSSI\" ,\"SGU\" ,\"LPI\" ,\"CEIX\" ,\"TTI\" ,\"NOA\" ,\"HESM\" ,\"OIS\" ,\"PDS\" ,\"TK\" ,\n",
    "                  \"TNP-PD\" ,\"TNP-PE\" ,\"NR\" ,\"TNP-PF\" ,\"NGL\" ,\"BORR\" ,\"AMR\" ,\"SJT\" ,\"NRP\" ,\"VIST\" ,\"OSG\" ,\"RNET\" ,\"TNP\" ,\"PBT\" ,\"NC\" ,\"TUSK\" ,\n",
    "                  \"GLOP\" ,\"SD\" ,\"EGY\" ,\"SLNG\" ,\"EXTN\" ,\"SBOW\" ,\"NGS\" ,\"FTK\" ,\"AMPY\" ,\"GEOS\" ,\"FET\" ,\"SND\" ,\"DLNG\" ,\"KLXE\" ,\"RNGR\" ,\"SMLP\" ,\n",
    "                  \"MMLP\" ,\"DLNG-PB\" ,\"BPT\" ,\"CCLP\" ,\"PRT\" ,\"PHX\" ,\"GIFI\" ,\"MVO\" ,\"NINE\" ,\"VOC\" ,\"CRT\" ,\"NRT\" ,\"DWSN\" ,\"NNA\" ,\"PVL\" ,\"IO\" ,\n",
    "                  \"CELP\" ,\"ICD\" ,\"MARPS\" ,\"MTR\" ,\"DCP-PB\" ,\"TGP-PA\" ,\"NS-PC\" ,\"ALIN-PA\" ,\"ALIN-PE\" ,\"EP-PC\" ,\"GLOP-PA\" ,\"CEQP-P\" ,\"HMLP-PA\" ,\n",
    "                  \"ALIN-PB\" ,\"DCP-PC\" ,\"NS-PA\" ,\"VAL\" ,\"TGP-PB\" ,\"DLNG-PA\" ,\"GLP-PB\" ,\"GLP-PA\" ,\"GMLPP\" ,\"UROY\"]\n",
    "\n",
    "for ticker in energy_tickers:\n",
    "    # For each ticker, navigate to the page that holds Seeking Alpha's links to transcripts for that include that ticker\n",
    "    page = 'https://seekingalpha.com/search?q=' + ticker + '&type=keyword&tab=transcripts'\n",
    "    driver.get(page)\n",
    "    # Pause for a moment, then send the page code off to beautifulsoup\n",
    "    time.sleep(random.randint(5,20)/10)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    # For each ticker, we want to collect a list of links to the transcipt data for each quarter, the quarter in question,\n",
    "    # and corresponding date. We initialize those lists here.\n",
    "    links = []\n",
    "    quarters = []\n",
    "    dates = []\n",
    "    # Collecting all the links from the first transcript page for each ticker:\n",
    "    for each in soup.find_all(\"div\",{'class':'item-details'}):\n",
    "        if (operator.contains(each.find('a').text, \"Earnings Call Transcript\")) & (operator.contains(each.a.text, ticker)):\n",
    "            links.append(\"https://seekingalpha.com\" + each.find('a').get(\"href\"))\n",
    "            calldate = each.find('div',{'class':'item-metadata'}).text.replace('\\n', \"\").replace(ticker, \"\").replace('SA Transcripts', \"\")\n",
    "            for day in ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']:\n",
    "                if (operator.contains(calldate, day)):\n",
    "                    n = calldate.find(day)\n",
    "                    calldate = calldate[n:].strip()\n",
    "            dates.append(calldate)\n",
    "            if operator.contains(each.find('a').text.lower(), \"q1\"):\n",
    "                quarters.append('q1')\n",
    "            elif operator.contains(each.find('a').text.lower(), \"q2\"):\n",
    "                quarters.append('q2')\n",
    "            elif operator.contains(each.find('a').text.lower(), \"q3\"):\n",
    "                quarters.append('q3')\n",
    "            else:\n",
    "                quarters.append('q4')\n",
    "    \n",
    "    # Each page contains 10 links. We'd like to have 10 transcript links per ticker. \n",
    "    # Some transcripts reference other tickers, so not all 10 links on the initial page will be to transcripts of the \n",
    "    # ticker in question. In those scenarios we can dig into the next 10 transcript-link pages. \n",
    "    # If we can't get 10 links in the first 10 pages, we'll move on to the next ticker. \n",
    "    pagenum = 2\n",
    "    trans_num = len(links) \n",
    "    while (trans_num < 10) & (pagenum < 10):\n",
    "        page = 'https://seekingalpha.com/search?q=' + ticker + '&type=keyword&tab=transcripts' + '#page=' + str(pagenum)\n",
    "        driver.get(page)\n",
    "        time.sleep(random.randint(5,20)/10)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "        # Collecting all the links from the subsequent transcript pages for each ticker:\n",
    "        for each in soup.find_all(\"div\",{'class':'item-details'}):\n",
    "            if (operator.contains(each.find('a').text, \"Earnings Call Transcript\")) & (operator.contains(each.a.text, ticker)):\n",
    "                links.append(\"https://seekingalpha.com\" + each.find('a').get(\"href\"))\n",
    "                calldate = each.find('div',{'class':'item-metadata'}).text.replace('\\n', \"\").replace(ticker, \"\").replace('SA Transcripts', \"\")\n",
    "                for day in ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']:\n",
    "                    if (operator.contains(calldate, day)):\n",
    "                        n = calldate.find(day)\n",
    "                        calldate = calldate[n:].strip()\n",
    "                dates.append(calldate)\n",
    "                if operator.contains(each.find('a').text.lower(), \"q1\"):\n",
    "                    quarters.append('q1')\n",
    "                elif operator.contains(each.find('a').text.lower(), \"q2\"):\n",
    "                    quarters.append('q2')\n",
    "                elif operator.contains(each.find('a').text.lower(), \"q3\"):\n",
    "                    quarters.append('q3')\n",
    "                else:\n",
    "                    quarters.append('q4')\n",
    "        pagenum += 1 \n",
    "        trans_num = len(links)\n",
    "    \n",
    "    # A pretty hacky solution to get the ordinal dates converted to datetime:\n",
    "    blank = ''\n",
    "    dates = [blank.join(calldate.rsplit('th', 1)) for calldate in dates]\n",
    "    dates = [blank.join(calldate.rsplit('st', 1)) for calldate in dates]\n",
    "    dates = [blank.join(calldate.rsplit('nd', 1)) for calldate in dates]\n",
    "    dates = [blank.join(calldate.rsplit('rd', 1)) for calldate in dates]\n",
    "    dates = [calldate.replace('Augu ', 'August ') for calldate in dates] \n",
    "    dates = [calldate.replace('Satuay', 'Saturday') for calldate in dates]\n",
    "    dates = [calldate.replace('Moay', 'Monday') for calldate in dates]\n",
    "    dates = [calldate.replace('Suay', 'Sunday') for calldate in dates] \n",
    "    dates = [datetime.strptime(calldate, '%A, %B %d %Y') for calldate in dates]\n",
    "    dates2 = [datetime.strftime(calldate, '%Y-%m-%d') for calldate in dates]\n",
    "    \n",
    "    i=0\n",
    "    # Now that we have the links to (ideally) 10 call transcripts for the ticker,\n",
    "    # we can follow those lnks and pull down the transcripts.\n",
    "    for ind, name in enumerate(links):\n",
    "        page = links[ind]\n",
    "        driver.get(page)\n",
    "        time.sleep(0.5)\n",
    "        page_source = driver.page_source\n",
    "        trans_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        # Here's the transcript: \n",
    "        trans = trans_soup.find(\"div\",{'data-test-id':'content-container'})\n",
    "        \n",
    "        # We'll hard-code the quarter and date information into the filename:\n",
    "        filename = ticker + \"_\" + quarters[ind] + \"_\" +  dates2[ind][0:4] + dates2[ind][5:7] + dates2[ind][8:]\n",
    "\n",
    "        # Create one text file for each transcript:\n",
    "        file = open(r\"./energy_transcripts/\" + filename.lower() + \".txt\", 'w')\n",
    "        file.write(trans.text)\n",
    "        file.close\n",
    "        i+=1\n",
    "    \n",
    "    print(ticker, i, 'transcripts recieved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T01:32:05.783091Z",
     "iopub.status.busy": "2021-05-12T01:32:05.782862Z",
     "iopub.status.idle": "2021-05-12T01:38:17.862299Z",
     "shell.execute_reply": "2021-05-12T01:38:17.861248Z",
     "shell.execute_reply.started": "2021-05-12T01:32:05.783067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACOR 10 transcripts recieved\n",
      "CPIX 10 transcripts recieved\n",
      "NBRV 8 transcripts recieved\n",
      "TLGT 10 transcripts recieved\n",
      "PTIX 0 transcripts recieved\n",
      "AGTI 0 transcripts recieved\n",
      "HOWL 0 transcripts recieved\n",
      "AVAH 0 transcripts recieved\n",
      "VACC 0 transcripts recieved\n",
      "BDXB 0 transcripts recieved\n",
      "AKYA 0 transcripts recieved\n",
      "CHNGU 0 transcripts recieved\n",
      "PALI 0 transcripts recieved\n",
      "AGL 17 transcripts recieved\n",
      "DHR-PB 0 transcripts recieved\n",
      "BSX-PA 0 transcripts recieved\n",
      "BMEA 0 transcripts recieved\n",
      "RAIN 0 transcripts recieved\n",
      "TMCI 0 transcripts recieved\n",
      "RXRX 0 transcripts recieved\n",
      "ELAT 0 transcripts recieved\n",
      "NUWE 0 transcripts recieved\n",
      "VECT 0 transcripts recieved\n",
      "PRVA 0 transcripts recieved\n"
     ]
    }
   ],
   "source": [
    "# Tickers collected by sector using Yahoo Finanace (https://finance.yahoo.com/screener/new)\n",
    "health_tickers = [\"JNJ\" ,\"UNH\" ,\"PFE\" ,\"ABT\" ,\"ABBV\" ,\"NVS\" ,\"MRK\" ,\"TMO\" ,\"LLY\" ,\"DHR\" ,\n",
    "                  \"MDT\" ,\"NVO\" ,\"DHR-PA\" ,\"AMGN\" ,\"BMY\" ,\"AZN\" ,\"SNY\" ,\"CVS\" ,\"ISRG\" ,\"ANTM\" ,\n",
    "                  \"SYK\" ,\"GSK\" ,\"CI\" ,\"GILD\" ,\"ZTS\" ,\"HCA\" ,\"BDX\" ,\"MRNA\" ,\"BSX\" ,\"HUM\" ,\n",
    "                  \"EW\" ,\"VRTX\" ,\"ILMN\" ,\"REGN\" ,\"TAK\" ,\"PHG\" ,\"WBA\" ,\"ALGN\" ,\"IDXX\" ,\"BAX\" ,\n",
    "                  \"IQV\" ,\"BIIB\" ,\"A\" ,\"BNTX\" ,\"VEEV\" ,\"CNC\" ,\"ALXN\" ,\"ZBH\" ,\"DXCM\" ,\"ALC\" ,\n",
    "                  \"MTD\" ,\"MCK\" ,\"BGNE\" ,\"RMD\" ,\"LH\" ,\"RPRX\" ,\"SGEN\" ,\"GMAB\" ,\"WST\" ,\"ABC\" ,\n",
    "                  \"CERN\" ,\"FMS\" ,\"TDOC\" ,\"COO\" ,\"HZNP\" ,\"NVCR\" ,\"WAT\" ,\"TFX\" ,\"SNN\" ,\"BIO-B\" ,\n",
    "                  \"DGX\" ,\"INCY\" ,\"CTLT\" ,\"BIO\" ,\"STE\" ,\"GRFS\" ,\"VTRS\" ,\"HOLX\" ,\"CRL\" ,\"PODD\" ,\n",
    "                  \"CAH\" ,\"PPD\" ,\"TECH\" ,\"MOH\" ,\"TXG\" ,\"ALNY\" ,\"PKI\" ,\"XRAY\" ,\"ELAN\" ,\"BMRN\" ,\n",
    "                  \"GDRX\" ,\"OSH\" ,\"DVA\" ,\"GH\" ,\"ABMD\" ,\"ARGX\" ,\"UHS\" ,\"MASI\" ,\"ICLR\" ,\"RDY\" ,\n",
    "                  \"NVAX\" ,\"TEVA\" ,\"HSIC\" ,\"PRAH\" ,\"QGEN\" ,\"BRKR\" ,\"RGEN\" ,\"PEN\" ,\"BHC\" ,\"JAZZ\" ,\n",
    "                  \"MRVI\" ,\"CGC\" ,\"UTHR\" ,\"EHC\" ,\"SYNH\" ,\"NBIX\" ,\"AMED\" ,\"NTRA\" ,\"RARE\" ,\"ABCL\" ,\n",
    "                  \"CHE\" ,\"MRTX\" ,\"BBIO\" ,\"EXEL\" ,\"HRC\" ,\"GMED\" ,\"NVST\" ,\"ASND\" ,\"CHNG\" ,\"THC\" ,\n",
    "                  \"ARWR\" ,\"SHC\" ,\"DNLI\" ,\"HALO\" ,\"RCM\" ,\"LHCG\" ,\"IART\" ,\"IBRX\" ,\"OMCL\" ,\"SRPT\" ,\n",
    "                  \"MEDP\" ,\"PRGO\" ,\"ACHC\" ,\"SGFY\" ,\"HQY\" ,\"NVTA\" ,\"ONEM\" ,\"NVRO\" ,\"VIR\" ,\"SEM\" ,\n",
    "                  \"IONS\" ,\"ALLK\" ,\"BPMC\" ,\"INSP\" ,\"TWST\" ,\"NEOG\" ,\"ADPT\" ,\"SWAV\" ,\"GLPG\" ,\"ALHC\" ,\n",
    "                  \"APHA\" ,\"INOV\" ,\"MPLN\" ,\"OCDX\" ,\"PACB\" ,\"NARI\" ,\"SDGR\" ,\"ABCM\" ,\"ENSG\" ,\"OSCR\" ,\n",
    "                  \"QDEL\" ,\"PGNY\" ,\"CERT\" ,\"ICUI\" ,\"PINC\" ,\"BHVN\" ,\"AMN\" ,\"BEAM\" ,\"ALLO\" ,\"CVET\" ,\n",
    "                  \"CNMD\" ,\"LIVN\" ,\"HCM\" ,\"INNV\" ,\"AGIO\" ,\"CMD\" ,\"GKOS\" ,\"APLS\" ,\"ARNA\" ,\"MMSI\" ,\n",
    "                  \"LEGN\" ,\"NUVA\" ,\"PDCO\" ,\"ALKS\" ,\"SWTX\" ,\"ACAD\" ,\"CLOV\" ,\"NKTR\" ,\"INSM\" ,\"EBS\" ,\n",
    "                  \"AMWL\" ,\"SDC\" ,\"OPCH\" ,\"HAE\" ,\"TPTX\" ,\"SANA\" ,\"ITGR\" ,\"BLI\" ,\"SGRY\" ,\"ARVN\" ,\n",
    "                  \"MOR\" ,\"INMD\" ,\"TARO\" ,\"PTCT\" ,\"PCRX\" ,\"OPK\" ,\"ITCI\" ,\"BFLY\" ,\"TIL\" ,\"MDRX\" ,\n",
    "                  \"OMI\" ,\"SEER\" ,\"MGLN\" ,\"GBT\" ,\"PBH\" ,\"ALXO\" ,\"MD\" ,\"ACCD\" ,\"BCRX\" ,\"TLRY\" ,\n",
    "                  \"AXNX\" ,\"HCSG\" ,\"HCAT\" ,\"OM\" ,\"IRTC\" ,\"EDIT\" ,\"PHR\" ,\"RVMD\" ,\"AVNS\" ,\"RUBY\" ,\n",
    "                  \"MYGN\" ,\"MCRB\" ,\"DRNA\" ,\"MODV\" ,\"DCPH\" ,\"NGM\" ,\"BLUE\" ,\"MDGL\" ,\"RCUS\" ,\"CCXI\" ,\n",
    "                  \"PRLD\" ,\"FGEN\" ,\"IGMS\" ,\"AVIR\" ,\"MGNX\" ,\"GBIO\" ,\"MYOV\" ,\"SILK\" ,\"ADCT\" ,\"LMNX\" ,\n",
    "                  \"IRWD\" ,\"IMCR\" ,\"CYH\" ,\"EVH\" ,\"ACB\" ,\"CYTK\" ,\"KURA\" ,\"REPL\" ,\"EAR\" ,\"IMVT\" ,\"ARQT\" ,\n",
    "                  \"PMVP\" ,\"CSII\" ,\"AMTI\" ,\"DSGN\" ,\"USPH\" ,\"LUNG\" ,\"ADUS\" ,\"SGMO\" ,\"RGNX\" ,\"QURE\" ,\"NFH\" ,\n",
    "                  \"INGN\" ,\"EWTX\" ,\"CCCC\" ,\"KRON\" ,\"TBIO\" ,\"ALVR\" ,\"IMGN\" ,\"PGEN\" ,\"ZYME\" ,\"ALEC\" ,\"STTK\" ,\n",
    "                  \"INO\" ,\"ZEAL\" ,\"CDXS\" ,\"PETQ\" ,\"CGEM\" ,\"ENDP\" ,\"CRY\" ,\"CMPS\" ,\"NXGN\" ,\"NRC\" ,\"BKD\" ,\n",
    "                  \"ACRS\" ,\"ATEC\" ,\"TVTY\" ,\"RPTX\" ,\"INVA\" ,\"YMAB\" ,\"STOK\" ,\"PLRX\" ,\"ATRI\" ,\"PNTG\" ,\"CRTX\" ,\n",
    "                  \"SNDL\" ,\"ATRA\" ,\"PRAX\" ,\"ALGS\" ,\"KNTE\" ,\"ENTA\" ,\"MRSN\" ,\"CNST\" ,\"SRRK\" ,\"KNSA\" ,\"HNGR\" ,\n",
    "                  \"RAD\" ,\"APR\" ,\"VREX\" ,\"PHAT\" ,\"PRTA\" ,\"BDTX\" ,\"AKRO\" ,\"ANGO\" ,\"FDMT\" ,\"RXDX\" ,\"PCVX\" ,\n",
    "                  \"VOR\" ,\"PASG\" ,\"NTUS\" ,\"AMPH\" ,\"AMRX\" ,\"ORIC\" ,\"OLMA\" ,\"OFIX\" ,\"NKTX\" ,\"HEXO\" ,\"HSTM\" ,\n",
    "                  \"VIVO\" ,\"TSHA\" ,\"DYN\" ,\"BOLT\" ,\"PHVS\" ,\"BVS\" ,\"MESO\" ,\"TCRR\" ,\"GRCL\" ,\"COLL\" ,\"ADAP\" ,\n",
    "                  \"ANNX\" ,\"SRDX\" ,\"GTHX\" ,\"RFL\" ,\"VYNE\" ,\"KDNY\" ,\"SNDX\" ,\"HARP\" ,\"ANAB\" ,\"OSUR\" ,\n",
    "                  \"EPZM\" ,\"CALT\" ,\"BCYC\" ,\"ATHA\" ,\"DBVT\" ,\"ACHL\" ,\"ZIOP\" ,\"ORTX\" ,\"GOSS\" ,\"LXRX\" ,\n",
    "                  \"ISEE\" ,\"KDMN\" ,\"CO\" ,\"PETS\" ,\"FNCH\" ,\"IDYA\" ,\"SPNE\" ,\"RLMD\" ,\"KPTI\" ,\"GTS\" ,\"CLVS\" ,\n",
    "                  \"MGTX\" ,\"TARS\" ,\"ANIK\" ,\"TLMD\" ,\"OGI\" ,\"ICPT\" ,\"RIGL\" ,\"CRNX\" ,\"CUTR\" ,\"PSTX\" ,\"CTMX\" ,\n",
    "                  \"XBIT\" ,\"NBTX\" ,\"EVLO\" ,\"VAPO\" ,\"DTIL\" ,\"OYST\" ,\"HOOK\" ,\"AKUS\" ,\"ANGN\" ,\"CPSI\" ,\"PRVB\" ,\n",
    "                  \"SPPI\" ,\"MTEM\" ,\"AMYT\" ,\"ARAY\" ,\"GERN\" ,\"SGTX\" ,\"ATNX\" ,\"GRTS\" ,\"VIRX\" ,\"SLDB\" ,\"SIEN\" ,\n",
    "                  \"TERN\" ,\"PBYI\" ,\"INZY\" ,\"OPT\" ,\"TXMD\" ,\"FUSN\" ,\"AVRO\" ,\"SPRB\" ,\"BCEL\" ,\"SPRO\" ,\"FREQ\" ,\n",
    "                  \"FLDM\" ,\"AFIB\" ,\"IPHA\" ,\"BDSI\" ,\"APYX\" ,\"UTMD\" ,\"JNCE\" ,\"ORPH\" ,\"SYRS\" ,\"FRLN\" ,\"KLDO\" ,\n",
    "                  \"LVTX\" ,\"KALA\" ,\"BLU\" ,\"SQZ\" ,\"CSLT\" ,\"FIXX\" ,\"KMDA\" ,\"CBAY\" ,\"NH\" ,\"IVC\" ,\"AUTL\" ,\"COGT\" ,\n",
    "                  \"KZR\" ,\"IMUX\" ,\"UBX\" ,\"ABUS\" ,\"CHMA\" ,\"CABA\" ,\"OVID\" ,\"NXTC\" ,\"NUVB\" ,\"TCDA\" ,\"LHDX\" ,\"INFI\" ,\n",
    "                  \"OSMT\" ,\"LCI\" ,\"SRGA\" ,\"NCNA\" ,\"DBTX\" ,\"GNFT\" ,\"OBSV\" ,\"XERS\" ,\"MIST\" ,\"VYGR\" ,\"OPTN\" ,\n",
    "                  \"SBBP\" ,\"SCPH\" ,\"ERYP\" ,\"CCM\" ,\"CALA\" ,\"SIOX\" ,\"GLTO\" ,\"ENZ\" ,\"ASMB\" ,\"IMRA\" ,\"ODT\" ,\"OTIC\" ,\n",
    "                  \"IFRX\" ,\"CSU\" ,\"APRE\" ,\"SVRA\" ,\"APTX\" ,\"CYCN\" ,\"ARAV\" ,\"ECOR\" ,\"GEG\" ,\"TRIB\" ,\"ALNA\" ,\n",
    "                  \"ACOR\" ,\"CPIX\" ,\"NBRV\" ,\"TLGT\" ,\"PTIX\" ,\"AGTI\" ,\"HOWL\" ,\"AVAH\" ,\"VACC\" ,\"BDXB\" ,\"AKYA\" ,\n",
    "                  \"CHNGU\" ,\"PALI\" ,\"AGL\" ,\"DHR-PB\" ,\"BSX-PA\" ,\"BMEA\" ,\"RAIN\" ,\"TMCI\" ,\"RXRX\" ,\"ELAT\" ,\"NUWE\" ,\"VECT\" ,\"PRVA\"]\n",
    "\n",
    "for ticker in health_tickers:\n",
    "    # For each ticker, navigate to the page that holds Seeking Alpha's links for transcripts that include that ticker\n",
    "    page = 'https://seekingalpha.com/search?q=' + ticker + '&type=keyword&tab=transcripts'\n",
    "    driver.get(page)\n",
    "    # Pause for a moment, then send the page code off to beautifulsoup\n",
    "    time.sleep(random.randint(5,20)/10)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    # For each ticker, we want to collect a list of links to the transcipt data for each quarter, the quarter in question,\n",
    "    # and corresponding date. We initialize those lists here.\n",
    "    links = []\n",
    "    quarters = []\n",
    "    dates = []\n",
    "    # Collecting all the links from the first transcript page for each ticker:\n",
    "    for each in soup.find_all(\"div\",{'class':'item-details'}):\n",
    "        if (operator.contains(each.find('a').text, \"Earnings Call Transcript\")) & (operator.contains(each.a.text, ticker)):\n",
    "            links.append(\"https://seekingalpha.com\" + each.find('a').get(\"href\"))\n",
    "            calldate = each.find('div',{'class':'item-metadata'}).text.replace('\\n', \"\").replace(ticker, \"\").replace('SA Transcripts', \"\")\n",
    "            calldate = calldate.replace('S Transcripts', \"\")\n",
    "            for day in ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']:\n",
    "                if (operator.contains(calldate, day)):\n",
    "                    n = calldate.find(day)\n",
    "                    calldate = calldate[n:].strip()\n",
    "            dates.append(calldate)\n",
    "            if operator.contains(each.find('a').text.lower(), \"q1\"):\n",
    "                quarters.append('q1')\n",
    "            elif operator.contains(each.find('a').text.lower(), \"q2\"):\n",
    "                quarters.append('q2')\n",
    "            elif operator.contains(each.find('a').text.lower(), \"q3\"):\n",
    "                quarters.append('q3')\n",
    "            else:\n",
    "                quarters.append('q4')\n",
    "    \n",
    "    # Each page contains 10 links. We'd like to have 10 transcript links per ticker. \n",
    "    # Some transcripts reference other tickers, so not all 10 links on the initial page will be to transcripts of the \n",
    "    # ticker in question. In those scenarios we can dig into the next 10 transcript-link pages. \n",
    "    # If we can't get 10 links in the first 10 pages, we'll move on to the next ticker. \n",
    "    pagenum = 2\n",
    "    trans_num = len(links) \n",
    "    while (trans_num < 10) & (pagenum < 10):\n",
    "        page = 'https://seekingalpha.com/search?q=' + ticker + '&type=keyword&tab=transcripts' + '#page=' + str(pagenum)\n",
    "        driver.get(page)\n",
    "        time.sleep(random.randint(5,20)/10)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "        # Collecting all the links from the subsequent transcript pages for each ticker:\n",
    "        for each in soup.find_all(\"div\",{'class':'item-details'}):\n",
    "            if (operator.contains(each.find('a').text, \"Earnings Call Transcript\")) & (operator.contains(each.a.text, ticker)):\n",
    "                links.append(\"https://seekingalpha.com\" + each.find('a').get(\"href\"))\n",
    "                calldate = each.find('div',{'class':'item-metadata'}).text.replace('\\n', \"\").replace(ticker, \"\").replace('SA Transcripts', \"\")\n",
    "                calldate = calldate.replace('S Transcripts', \"\")\n",
    "                for day in ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']:\n",
    "                    if (operator.contains(calldate, day)):\n",
    "                        n = calldate.find(day)\n",
    "                        calldate = calldate[n:].strip()\n",
    "                dates.append(calldate)\n",
    "                if operator.contains(each.find('a').text.lower(), \"q1\"):\n",
    "                    quarters.append('q1')\n",
    "                elif operator.contains(each.find('a').text.lower(), \"q2\"):\n",
    "                    quarters.append('q2')\n",
    "                elif operator.contains(each.find('a').text.lower(), \"q3\"):\n",
    "                    quarters.append('q3')\n",
    "                else:\n",
    "                    quarters.append('q4')\n",
    "        pagenum += 1 \n",
    "        trans_num = len(links)\n",
    "    \n",
    "    # A pretty hacky solution to get the ordinal dates converted to datetime:\n",
    "    blank = ''\n",
    "    dates = [blank.join(calldate.rsplit('th', 1)) for calldate in dates]\n",
    "    dates = [blank.join(calldate.rsplit('st', 1)) for calldate in dates]\n",
    "    dates = [blank.join(calldate.rsplit('nd', 1)) for calldate in dates]\n",
    "    dates = [blank.join(calldate.rsplit('rd', 1)) for calldate in dates]\n",
    "    dates = [calldate.replace('Augu ', 'August ') for calldate in dates] \n",
    "    dates = [calldate.replace('Satuay', 'Saturday') for calldate in dates]\n",
    "    dates = [calldate.replace('Moay', 'Monday') for calldate in dates]\n",
    "    dates = [calldate.replace('Suay', 'Sunday') for calldate in dates] \n",
    "    dates = [datetime.strptime(calldate, '%A, %B %d %Y') for calldate in dates]\n",
    "    dates2 = [datetime.strftime(calldate, '%Y-%m-%d') for calldate in dates]\n",
    "    \n",
    "    i=0\n",
    "    # Now that we have the links to (ideally) 10 call transcripts for the ticker,\n",
    "    # we can follow those lnks and pull down the transcripts.\n",
    "    for ind, name in enumerate(links):\n",
    "        page = links[ind]\n",
    "        driver.get(page)\n",
    "        time.sleep(0.5)\n",
    "        page_source = driver.page_source\n",
    "        trans_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        # Here's the transcript: \n",
    "        trans = trans_soup.find(\"div\",{'data-test-id':'content-container'})\n",
    "        \n",
    "        # We'll hard-code the quarter and date information into the filename:\n",
    "        filename = ticker + \"_\" + quarters[ind] + \"_\" +  dates2[ind][0:4] + dates2[ind][5:7] + dates2[ind][8:]\n",
    "\n",
    "        # Create one text file for each transcript:\n",
    "        file = open(r\"./health_transcripts/\" + filename.lower() + \".txt\", 'w')\n",
    "        file.write(trans.text)\n",
    "        file.close\n",
    "        i+=1\n",
    "    \n",
    "    print(ticker, i, 'transcripts recieved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T17:58:07.677162Z",
     "iopub.status.busy": "2021-05-10T17:58:07.676828Z",
     "iopub.status.idle": "2021-05-10T18:08:09.454787Z",
     "shell.execute_reply": "2021-05-10T18:08:09.454155Z",
     "shell.execute_reply.started": "2021-05-10T17:58:07.677130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVNW 10 transcripts recieved\n",
      "CTG 10 transcripts recieved\n",
      "GSIT 10 transcripts recieved\n",
      "ASYS 10 transcripts recieved\n",
      "SREV 10 transcripts recieved\n",
      "SNCR 10 transcripts recieved\n",
      "SPI 0 transcripts recieved\n",
      "CTK 10 transcripts recieved\n",
      "PCTI 10 transcripts recieved\n",
      "RELL 10 transcripts recieved\n",
      "RCAT 0 transcripts recieved\n",
      "TESS 10 transcripts recieved\n",
      "UTSI 10 transcripts recieved\n",
      "MINDP 0 transcripts recieved\n",
      "SEAC 10 transcripts recieved\n",
      "MIND 12 transcripts recieved\n",
      "KNBE 0 transcripts recieved\n",
      "SABRP 0 transcripts recieved\n",
      "IIVIP 0 transcripts recieved\n",
      "APP 12 transcripts recieved\n",
      "VZIO 0 transcripts recieved\n",
      "CTLP 1 transcripts recieved\n",
      "STEM 6 transcripts recieved\n",
      "COIN 0 transcripts recieved\n",
      "AVGOP 0 transcripts recieved\n",
      "ALKT 0 transcripts recieved\n",
      "PATH 0 transcripts recieved\n",
      "DV 10 transcripts recieved\n"
     ]
    }
   ],
   "source": [
    "tech_tickers = [\"AAPL\" ,\"MSFT\" ,\"TSM\" ,\"NVDA\" ,\"ASML\" ,\"ORCL\" ,\"INTC\" ,\"ADBE\" ,\"CSCO\" ,\"CRM\" ,\"ACN\" ,\"AVGO\" ,\"TXN\" ,\"SAP\" ,\"QCOM\" ,\"SHOP\" ,\"IBM\" ,\"SONY\" ,\"AMAT\" ,\n",
    "                \"INTU\" ,\"SQ\" ,\"NOW\" ,\"MU\" ,\"AMD\" ,\"FIS\" ,\"LRCX\" ,\"UBER\" ,\"FISV\" ,\"INFY\" ,\"DELL\" ,\"VMW\" ,\"ADSK\" ,\"ADI\" ,\"WDAY\" ,\"TEAM\" ,\"NXPI\" ,\"KLAC\" ,\n",
    "                \"ERIC\" ,\"HPQ\" ,\"TEL\" ,\"CRWD\" ,\"CTSH\" ,\"MCHP\" ,\"PLTR\" ,\"WIT\" ,\"DOCU\" ,\"SNPS\" ,\"CDNS\" ,\"GLW\" ,\"FTNT\" ,\"STM\" ,\"PANW\" ,\"MSI\" ,\"MRVL\" ,\"OKTA\" ,\"XLNX\" ,\n",
    "                \"SWKS\" ,\"ANSS\" ,\"NOK\" ,\"GRMN\" ,\"KEYS\" ,\"FTV-PA\" ,\"EPAM\" ,\"MXIM\" ,\"VRSN\" ,\"ZBRA\" ,\"ANET\" ,\"CDW\" ,\"FTV\" ,\"CAJ\" ,\"WORK\" ,\"FLT\" ,\"RNG\" ,\"GIB\" ,\n",
    "                \"HUBS\" ,\"ZS\" ,\"DDOG\" ,\"WDC\" ,\"STX\" ,\"HPE\" ,\"TER\" ,\"QRVO\" ,\"UMC\" ,\"APH\" ,\"BR\" ,\"TRMB\" ,\"STNE\" ,\"SPLK\" ,\"PAYC\" ,\"LOGI\" ,\"SSNC\" ,\n",
    "                \"AKAM\" ,\"NTAP\" ,\"COUP\" ,\"CHKP\" ,\"CLVT\" ,\"UI\" ,\"TYL\" ,\"LYFT\" ,\"ZEN\" ,\"TDY\" ,\"WIX\" ,\"ON\" ,\"PTC\" ,\"NUAN\" ,\"MPWR\" ,\"ENTG\" ,\"CTXS\" ,\"LDOS\" ,\"ASX\" ,\n",
    "                \"NICE\" ,\"FICO\" ,\"CGNX\" ,\"PAGS\" ,\"GDDY\" ,\"AFRM\" ,\"BSY\" ,\"CDAY\" ,\"OTEX\" ,\"NLOK\" ,\"JKHY\" ,\"BKI\" ,\"FFIV\" ,\"TUYA\" ,\"SEDG\" ,\"BILL\" ,\"IPGP\" ,\"AVLR\" ,\n",
    "                \"CREE\" ,\"OLED\" ,\"DBX\" ,\"DOX\" ,\"MKSI\" ,\"DLB\" ,\"PSFE\" ,\"PFPT\" ,\"MCFE\" ,\"PEGA\" ,\"ESTC\" ,\"PCTY\" ,\"RUN\" ,\"FLEX\" ,\"ST\" ,\"KC\" ,\"JNPR\" ,\"G\" ,\"WEX\" ,\n",
    "                \"AZPN\" ,\"DXC\" ,\"ARW\" ,\"GLOB\" ,\"MANH\" ,\"TIXT\" ,\"GWRE\" ,\"CIEN\" ,\"CNXC\" ,\"JBL\" ,\"FSLR\" ,\"FLIR\" ,\"PLAN\" ,\"LPL\" ,\"LSPD\" ,\"EEFT\" ,\"BRKS\" ,\"COMP\" ,\"LSCC\" ,\n",
    "                \"FOUR\" ,\"SMAR\" ,\"CDK\" ,\"CACI\" ,\"LAZR\" ,\"LFUS\" ,\"IIVI\" ,\"SNX\" ,\"COHR\" ,\"LITE\" ,\"NCR\" ,\"BMBL\" ,\"BL\" ,\"MSTR\" ,\"SLAB\" ,\"CLGX\" ,\"NTNX\" ,\"VNT\" ,\"DAVA\" ,\n",
    "                \"OCFT\" ,\"CRCT\" ,\"AI\" ,\"NATI\" ,\"TDC\" ,\"JCOM\" ,\"SWI\" ,\"NCNO\" ,\"DSGX\" ,\"AYX\" ,\"SAIC\" ,\"ACVA\" ,\"DQ\" ,\"QTWO\" ,\"CD\" ,\"PSTG\" ,\"DCT\" ,\"POWI\" ,\"RXT\" ,\"XRX\" ,\n",
    "                \"AMKR\" ,\"FSLY\" ,\"ALTR\" ,\"DOCN\" ,\"PRSP\" ,\"ALGM\" ,\"TTEC\" ,\"SYNA\" ,\"CCMP\" ,\"BB\" ,\"ASAN\" ,\"SONO\" ,\"CYBR\" ,\"FEYE\" ,\"NOVT\" ,\"CRUS\" ,\"ACIW\" ,\"AVT\" ,\"SWCH\" ,\n",
    "                \"OLO\" ,\"ATC\" ,\"SAIL\" ,\"API\" ,\"WK\" ,\"ALRM\" ,\"SMTC\" ,\"MDLA\" ,\"SABR\" ,\"MSP\" ,\"SPWR\" ,\"FROG\" ,\"QLYS\" ,\"TENB\" ,\"ENV\" ,\"JAMF\" ,\"NEWR\" ,\"DIOD\" ,\"VICR\" ,\n",
    "                \"MTSI\" ,\"VIAV\" ,\"ROG\" ,\"STMP\" ,\"SPSC\" ,\"VSH\" ,\"CLDR\" ,\"VSAT\" ,\"COMM\" ,\"ITRI\" ,\"MANT\" ,\"LPSN\" ,\"NSIT\" ,\"CRNC\" ,\"EXLS\" ,\"VNET\" ,\"BOX\" ,\"BLKB\" ,\"VRNT\" ,\n",
    "                \"RAMP\" ,\"AMBA\" ,\"FN\" ,\"CVLT\" ,\"EGHT\" ,\"ONTO\" ,\"NOVA\" ,\"PD\" ,\"EVTC\" ,\"BAND\" ,\"CRSR\" ,\"KLIC\" ,\"FORM\" ,\"TSEM\" ,\"ESE\" ,\"YALA\" ,\"CALX\" ,\"IRBT\" ,\"MIME\" ,\n",
    "                \"CSOD\" ,\"SANM\" ,\"PLXS\" ,\"DM\" ,\"NVMI\" ,\"MXL\" ,\"SATS\" ,\"SVMK\" ,\"CSIQ\" ,\"DDD\" ,\"SIMO\" ,\"RIOT\" ,\"PRFT\" ,\"MFGP\" ,\"AVYA\" ,\"SEMR\" ,\"BTRS\" ,\"RMBS\" ,\"HIMX\" ,\n",
    "                \"XPER\" ,\"MVIS\" ,\"VLDR\" ,\"PING\" ,\"UCTT\" ,\"PRGS\" ,\"NTCT\" ,\"KN\" ,\"EB\" ,\"MAXR\" ,\"GB\" ,\"ZUO\" ,\"SMCI\" ,\"ONTF\" ,\"DSP\" ,\"PRO\" ,\"OUST\" ,\"MEI\" ,\"OSIS\" ,\"EPAY\" ,\n",
    "                \"SYKE\" ,\"SUMO\" ,\"INFN\" ,\"RAAS\" ,\"PAR\" ,\"PLT\" ,\"TTMI\" ,\"YEXT\" ,\"DCBO\" ,\"CGNT\" ,\"COHU\" ,\"SPNS\" ,\"GPRO\" ,\"CNDT\" ,\"UIS\" ,\"ICHR\" ,\"VRNS\" ,\"CSGS\" ,\"JKS\" ,\n",
    "                \"MTLS\" ,\"EXTR\" ,\"FORTY\" ,\"QADA\" ,\"PLUS\" ,\"ACLS\" ,\"MODN\" ,\"QADB\" ,\"MLAB\" ,\"CNXN\" ,\"RDWR\" ,\"LINX\" ,\"FARO\" ,\"OPRA\" ,\"AGYS\" ,\"GSKY\" ,\"DBD\" ,\"VCRA\" ,\"NTGR\" ,\n",
    "                \"CEVA\" ,\"PI\" ,\"SGH\" ,\"BHE\" ,\"SSYS\" ,\"VECO\" ,\"IMOS\" ,\"SCWX\" ,\"CLS\" ,\"CTS\" ,\"AUDC\" ,\"LASR\" ,\"IIIV\" ,\"EBIX\" ,\"MX\" ,\"ADTN\" ,\"PLAB\" ,\"UEIC\" ,\"SCSC\" ,\"INSG\" ,\n",
    "                \"MGIC\" ,\"AOSL\" ,\"HLIT\" ,\"ATEN\" ,\"CASA\" ,\"PDFS\" ,\"ABST\" ,\"ECOM\" ,\"ZEPP\" ,\"AMSWA\" ,\"ALLT\" ,\"CMTL\" ,\"BCOV\" ,\"ITRN\" ,\"MAXN\" ,\"GILT\" ,\"SWIR\" ,\"DGII\" ,\n",
    "                \"VIOT\" ,\"DMRC\" ,\"HCKT\" ,\"SOL\" ,\"EBON\" ,\"NPTN\" ,\"ETWO\" ,\"CAMP\" ,\"VPG\" ,\"VOXX\" ,\"ZIXI\" ,\"AXTI\" ,\"LLNW\" ,\"DSPG\" ,\"VIAO\" ,\"VHC\" ,\"TUFN\" ,\"HBB\" ,\"SILC\" ,\n",
    "                \"UEPS\" ,\"DAKT\" ,\"CMCM\" ,\"MIXT\" ,\"WTRH\" ,\"CRNT\" ,\"SOS\" ,\"KVHI\" ,\"IMMR\" ,\"BELFA\" ,\"BELFB\" ,\"EXFO\" ,\"LYTS\" ,\"CIH\" ,\"OIIM\" ,\"SQNS\" ,\"AVNW\" ,\"CTG\" ,\"GSIT\" ,\n",
    "                \"ASYS\" ,\"SREV\" ,\"SNCR\" ,\"SPI\" ,\"CTK\" ,\"PCTI\" ,\"RELL\" ,\"RCAT\" ,\"TESS\" ,\"UTSI\" ,\"MINDP\" ,\"SEAC\" ,\"MIND\" ,\"KNBE\" ,\"SABRP\" ,\"IIVIP\" ,\"APP\" ,\"VZIO\" ,\"CTLP\" ,\n",
    "                \"STEM\" ,\"COIN\" ,\"AVGOP\" ,\"ALKT\" ,\"PATH\" ,\"DV\"]\n",
    "for ticker in tech_tickers:\n",
    "    # For each ticker, navigate to the page that holds Seeking Alpha's links for transcripts that include that ticker\n",
    "    page = 'https://seekingalpha.com/search?q=' + ticker + '&type=keyword&tab=transcripts'\n",
    "    driver.get(page)\n",
    "    # Pause for a moment, then send the page code off to beautifulsoup\n",
    "    time.sleep(random.randint(5,20)/10)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    # For each ticker, we want to collect a list of links to the transcipt data for each quarter, the quarter in question,\n",
    "    # and corresponding date. We initialize those lists here.\n",
    "    links = []\n",
    "    quarters = []\n",
    "    dates = []\n",
    "    # Collecting all the links from the first transcript page for each ticker:\n",
    "    for each in soup.find_all(\"div\",{'class':'item-details'}):\n",
    "        if (operator.contains(each.find('a').text, \"Earnings Call Transcript\")) & (operator.contains(each.a.text, ticker)):\n",
    "            links.append(\"https://seekingalpha.com\" + each.find('a').get(\"href\"))\n",
    "            calldate = each.find('div',{'class':'item-metadata'}).text.replace('\\n', \"\").replace(ticker, \"\").replace('SA Transcripts', \"\")\n",
    "            for day in ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']:\n",
    "                if (operator.contains(calldate, day)):\n",
    "                    n = calldate.find(day)\n",
    "                    calldate = calldate[n:].strip()\n",
    "            dates.append(calldate)\n",
    "            if operator.contains(each.find('a').text.lower(), \"q1\"):\n",
    "                quarters.append('q1')\n",
    "            elif operator.contains(each.find('a').text.lower(), \"q2\"):\n",
    "                quarters.append('q2')\n",
    "            elif operator.contains(each.find('a').text.lower(), \"q3\"):\n",
    "                quarters.append('q3')\n",
    "            else:\n",
    "                quarters.append('q4')\n",
    "    \n",
    "    # Each page contains 10 links. We'd like to have 10 transcript links per ticker. \n",
    "    # Some transcripts reference other tickers, so not all 10 links on the initial page will be to transcripts of the \n",
    "    # ticker in question. In those scenarios we can dig into the next 10 transcript-link pages. \n",
    "    # If we can't get 10 links in the first 10 pages, we'll move on to the next ticker. \n",
    "    pagenum = 2\n",
    "    trans_num = len(links) \n",
    "    while (trans_num < 10) & (pagenum < 10):\n",
    "        page = 'https://seekingalpha.com/search?q=' + ticker + '&type=keyword&tab=transcripts' + '#page=' + str(pagenum)\n",
    "        driver.get(page)\n",
    "        time.sleep(random.randint(5,20)/10)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "        # Collecting all the links from the subsequent transcript pages for each ticker:\n",
    "        for each in soup.find_all(\"div\",{'class':'item-details'}):\n",
    "            if (operator.contains(each.find('a').text, \"Earnings Call Transcript\")) & (operator.contains(each.a.text, ticker)):\n",
    "                links.append(\"https://seekingalpha.com\" + each.find('a').get(\"href\"))\n",
    "                calldate = each.find('div',{'class':'item-metadata'}).text.replace('\\n', \"\").replace(ticker, \"\").replace('SA Transcripts', \"\")\n",
    "                for day in ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']:\n",
    "                    if (operator.contains(calldate, day)):\n",
    "                        n = calldate.find(day)\n",
    "                        calldate = calldate[n:].strip()\n",
    "                dates.append(calldate)\n",
    "                if operator.contains(each.find('a').text.lower(), \"q1\"):\n",
    "                    quarters.append('q1')\n",
    "                elif operator.contains(each.find('a').text.lower(), \"q2\"):\n",
    "                    quarters.append('q2')\n",
    "                elif operator.contains(each.find('a').text.lower(), \"q3\"):\n",
    "                    quarters.append('q3')\n",
    "                else:\n",
    "                    quarters.append('q4')\n",
    "        pagenum += 1 \n",
    "        trans_num = len(links)\n",
    "    \n",
    "    # A pretty hacky solution to get the ordinal dates converted to datetime:\n",
    "    blank = ''\n",
    "    dates = [blank.join(calldate.rsplit('th', 1)) for calldate in dates]\n",
    "    dates = [blank.join(calldate.rsplit('st', 1)) for calldate in dates]\n",
    "    dates = [blank.join(calldate.rsplit('nd', 1)) for calldate in dates]\n",
    "    dates = [blank.join(calldate.rsplit('rd', 1)) for calldate in dates]\n",
    "    dates = [calldate.replace('Augu ', 'August ') for calldate in dates] \n",
    "    dates = [calldate.replace('Satuay', 'Saturday') for calldate in dates]\n",
    "    dates = [calldate.replace('Moay', 'Monday') for calldate in dates]\n",
    "    dates = [calldate.replace('Suay', 'Sunday') for calldate in dates] \n",
    "    dates = [datetime.strptime(calldate, '%A, %B %d %Y') for calldate in dates]\n",
    "    dates2 = [datetime.strftime(calldate, '%Y-%m-%d') for calldate in dates]\n",
    "    \n",
    "    i=0\n",
    "    # Now that we have the links to (ideally) 10 call transcripts for the ticker,\n",
    "    # we can follow those lnks and pull down the transcripts.\n",
    "    for ind, name in enumerate(links):\n",
    "        page = links[ind]\n",
    "        driver.get(page)\n",
    "        time.sleep(0.5)\n",
    "        page_source = driver.page_source\n",
    "        trans_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        # Here's the transcript: \n",
    "        trans = trans_soup.find(\"div\",{'data-test-id':'content-container'})\n",
    "        \n",
    "        # We'll hard-code the quarter and date information into the filename:\n",
    "        filename = ticker + \"_\" + quarters[ind] + \"_\" +  dates2[ind][0:4] + dates2[ind][5:7] + dates2[ind][8:]\n",
    "\n",
    "        # Create one text file for each transcript:\n",
    "        file = open(r\"./tech_transcripts/\" + filename.lower() + \".txt\", 'w')\n",
    "        file.write(trans.text)\n",
    "        file.close\n",
    "        i+=1\n",
    "    \n",
    "    print(ticker, i, 'transcripts recieved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis]",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
